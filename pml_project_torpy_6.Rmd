---
title       : Course Project
subtitle    : Practical Machine Learning
author      : Jason Torpy
output      : html_document
---
# Main Body

## Executive Summary
In this project, we seek prediction for the variable classe in weight lifting data. We reviewed several models, settling on the Random Forest as the best predictor. We found nearly 100% Accuracy with less than 1% out of sample error. Cross-validation was included (K=5, though cv is technically inherent to Random Forest). These outcomes, while not perfect, were superior to the other models and sufficient for our prediction requirements without excessive computing complexity

_Grading note: Word count is well under 2000, even with code, and there are 4 figures in the body of the report (Accuracy, C results, final fit model with OOB, and a chart of importance). The Appendices are optional and have additional figures should you wish to review them._

### Project Instructions

Predict outcomes (classe) using personal activity data. Data is from the Weight Lifting Dataset below, citations found there as well.
http://groupware.les.inf.puc-rio.br/har

Describe out the model was built, cross-validation, expected out of sample error, and the reason behind choices. Predict 20 test cases.

Publish to github repo with Rmd and resulting HTML as gh-page, with under 2000 words with fewer than 5 figures.

## Load and Explore Data

Preview of the data shows a training set of 19622 observations of 160 variables, with 20 observations in the test set. We need to do initial exploration of NAs for proper import. NAs, including blank and div/0 are cleaned on loading. That yields a first finding that about 100 variables have all NAs. While some are filled out in the example set, we will revisit and remove these.
We also find that the last column in the testing set is problem_id, which should be set as factor (in prepare data section below), and the last column in the training set is classe, our outcome (already a factor).


```{r prep, message=F,warning=F}
set.seed(1234)
par(mfrow=c(1,1))
library(knitr)
library(caret)
library(randomForest)

example_raw <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),na.strings=c("NA","","#DIV/0!"))
key_raw <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),na.strings=c("NA","","#DIV/0!"))

ex_allna <- table(sapply(example_raw, function(x) all(is.na(x))))
ex_anyna <- table(sapply(example_raw, function(x) any(is.na(x))))
key_allna <- table(sapply(key_raw, function(x) all(is.na(x))))
key_anyna <- table(sapply(key_raw, function(x) any(is.na(x))))

NAtable <- rbind(ex_allna,ex_anyna,key_allna,key_anyna)

```

## Prepare training data

Fortunately there is no apparent time-series or other barrier to random sampling, so we will proceed with that. A simple 70/30 split seems acceptable.

```{r training}

ex_part <- createDataPartition(example_raw$classe, p=0.7, list=FALSE)
ex_train <- example_raw[ex_part, ]
ex_test <- example_raw[-ex_part, ]
```

New dimensions are Training: `r dim(ex_train)` and Test: `r dim(ex_test)`

We expect many columns to drop out due to the high number of NAs. Looking for items near 0 variance to exclude will be a good start. This may eliminate some significant variables, but it is statistically unlikely.

```{r nzv}
zerovtemp <- nearZeroVar(ex_train, saveMetrics=TRUE)
ex_train2 <- ex_train[,zerovtemp$nzv==FALSE]
```

This leaves us with `r dim(ex_train2)[2]` variables. Surprisingly, many variables with limited non-NA observations were left in. Those should go, especially since the 100 all-NA columns in the 'key' data set provided seem to indicate they are non-significant. We also remove other variables we wouldn't expect to be correlated, eg timestamps and user names (col 1-6).

```{r NAs}
# remove first 6 as they are non-significant admin entries
ex_train3 <- ex_train2[,-(1:6)]
# remove columns with 90% or more NAs
ex_train3 <- ex_train3[, colSums(is.na(ex_train3)) < I(.9*nrow(ex_train3))]
# reorder data with outcome first, train and test:
ex_train3 <- ex_train3[,ncol(ex_train3):1]
ex_test2 <- ex_test[,ncol(ex_test):1] 
# Match test set columns to training columns
ex_test3 <- ex_test[,names(ex_train3)]
# returning to set problemid as factor and change order
key_raw2 <- key_raw[,ncol(key_raw):1] 
key_raw2$problem_id <- as.factor(key_raw$problem_id)
```

This treatment leaves us with `r dim(ex_train3)[2]` variables likely to give us the best predictions, including outcome classe, and the same for the train and test set.

Now we must find a good model for prediction.

## Model training and selection

We've used several models including random forest (rf), decision tree (rpart), and boosting (gbm). Each includes cross-validation up front, with K=5 (non-repeating). We might need cross-validation later, and we want to make good tradeoffs for computing power in terms of iteration, so we'll do it now. Similarly, The random forest is capped at `r 2*ncol(ex_train3)` leaves, twice the variables remaining including classe, which seems like enough.

```{r model1,cache=T,message=F,warning=F}
# reusable cross validation variable
tcon <- trainControl(method="cv",number=5) 
# decision tree with 5 options - 12.4MB
fit_dt <- train(classe~.,data=ex_train3,method="rpart",trControl=tcon)
# random forest - 12.3MB
fit_rf <- train(classe~.,data=ex_train3,method="rf",ntree=I(2*ncol(ex_train3)),trControl=tcon)
# boosting - 12.6MB
#fit_gb <- train(classe~.,data=ex_train3,trControl=tcon,method='gbm')
garbage <- capture.output(
  train <- fit_gb <- train(classe~.,data=ex_train3,trControl=tcon,method='gbm'))

```


Given we have three models set, we now move to generate predictions based on those models and prepare a Confusion Matrix to see how well the predictions performed.
```{r modelreview, cache=T}

pred_dt <- predict(fit_dt, newdata=ex_test3)
pred_rf <- predict(fit_rf, newdata=ex_test3)
pred_gb <- predict(fit_gb, newdata=ex_test3)

cm_dt <- confusionMatrix(pred_dt, ex_test3$classe)
cm_rf <- confusionMatrix(pred_rf, ex_test3$classe)
cm_gb <- confusionMatrix(pred_rf, ex_test3$classe)

ise_dt <- sum(pred_dt != ex_train3$classe)/length(ex_train3$classe)
ise_rf <- sum(pred_rf != ex_train3$classe)/length(ex_train3$classe)
ise_gb <- sum(pred_gb != ex_train3$classe)/length(ex_train3$classe)

out_a <- rbind(cm_dt$overall[1],cm_rf$overall[1],cm_gb$overall[1])
out_a <- cbind(out_a,c(ise_dt,ise_rf,ise_gb))
row.names(out_a) <- c("Tree","Forest","Boosting")
colnames(out_a) <- c("Accuracy","Sample Error")
out_a # displays accuracy and in sample error of each method.


rbind(cm_dt$table,cm_gb$table,cm_rf$table) # in/out of sample


```

We find above Accuracy for Decision Tree (CART), Generalized Boosting (GBM), and Random Forest, and tables of predictions, in order. GBM (`r round(cm_gb$overall[1],2)`) and Random Forest (`r round(cm_rf$overall[1],2)`) yieled equal results, which were better than the Decision tree (`r round(cm_dt$overall[1],2)`).

## Final Model Selection

Rather than doing additional manipulation by combining results or reducing computational compexity, we can accept what we have due to the high accuracy and acceptable complexity.

**The final selected model is Random Forest due to its higher accuracy and slightly faster run time.** Plot shows mtry/optimum sample at 27 variables. OOB error rate 0.79%, which seems pretty good, and that is on our test set, so so we'll have to re-run that on the entire set to get a truer estimation of out of sample error. Also shown is a plot of importance for all 52 predictors to review the impact of those many interactions.


```{r oose}
# re-knit full data set
ex_all3 <- data.frame(rbind(ex_train3,ex_test3))
pred_rf_all3 <- predict(fit_rf, newdata=ex_all3)
# calculate oose to be displayed below, predictions of chosen model versus all values, not just test
oose <- sum(pred_rf_all3 != ex_all3$classe)/length(ex_all3$classe)

```

**In calculations above, we find the Out of Sample error is `r paste(round(oose*100,2),"%")`**

Finally, We need to test and see how things came out against the validation data.

```{r finalmodel}
fit_rf$finalModel
plot(fit_rf$finalModel$importance)
```



## Model Testing

To test the model, we ran the chosen Random Forest model against the raw data. Due to space constraints, those figures are moved to Appendix 3.

```{r testing}
pred_key2 <- predict(fit_rf,newdata=key_raw2)
pred_key2_results <- data.frame(problem_id=key_raw2$problem_id,RFprediction=pred_key2)
```

## Conclusion

Given 160 variables to interpret, we were able to remove administrative entries, missing values, and near-zero-impact values. After narrowing to 52 predictors, we tried 3 models with 5-fold cross-validation, eventually choosing Random Forest. The RF approach had near-100% expected accuracy and under 1% out of sample error. Reviewing sample error is a suggested next step as well as gathering additional data on the NA dimensions.


# Appendices - Optional for graders' convenience

## Appendix 1: Model results

Model results, plot, and Confusion Matrix for each of our three explored models - CART/Decision Tree, Random Forest (selected), and finally Boosting.

```{r app1}
fit_dt
plot(fit_dt)
cm_dt

fit_rf
plot(fit_rf)
cm_rf

fit_gb
plot(fit_gb)
cm_gb
```

## Appendix 2: Exploration Results

Dimensions of the downloaded data (testing 'key' then training 'example') and a chart of NAs in each area (ex=example/training data, key = validation/testing data)
```{r app2}
dim(key_raw)
dim(example_raw)
NAtable
```

## Appendix 3: Final Test Results

Below are listed the final predictions of the chosen model against the raw testing 'key' data.

```{r App3}
pred_key2_results
```
